{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "656e0c10-b548-4802-afe0-13fd5acbd776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: TRANSFORMERS_CACHE=/scratch/alif\n",
      "env: HF_DATASETS_CACHE=/scratch/alif\n"
     ]
    }
   ],
   "source": [
    "%env TRANSFORMERS_CACHE=/scratch/alif\n",
    "%env HF_DATASETS_CACHE=/scratch/alif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d5c2b8f-e4c1-4c46-9f9e-1b8a15d1eab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')  # This adds the parent directory to the path\n",
    "\n",
    "from model import GPT, GPTConfig  # Now you can import your classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9b6ab74-addd-4a39-9ac8-afdabb918301",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import math\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import inspect\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "from model import GPTConfig, GPT\n",
    "\n",
    "from finetuning.lora import add_lora\n",
    "from finetuning.utils import tie_weights, get_lora_params, get_lora_state_dict, get_oft_params, get_oft_state_dict\n",
    "from finetuning.oft import inject_trainable_oft, inject_trainable_oft_conv, inject_trainable_oft_extended, inject_trainable_oft_with_norm\n",
    "\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fb83be91-cebe-4965-81d3-c41541dc0140",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8226d17-be15-44b3-b9c1-628b58382129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, Dict, List, Optional, Set, Tuple, Type, Union\n",
    "from oft import OFTInjectedLinear, OFTInjectedConv2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23bfdc8c-d827-4fc6-85c8-559daf44f8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dir = 'out'\n",
    "eval_interval = 2000\n",
    "log_interval = 1\n",
    "eval_iters = 200\n",
    "eval_only = False \n",
    "always_save_checkpoint = True \n",
    "\n",
    "wandb_log = False \n",
    "wandb_project = 'owt'\n",
    "wandb_run_name = 'gpt2' \n",
    "\n",
    "dataset = 'openwebtext'\n",
    "gradient_accumulation_steps = 5 * 8 \n",
    "batch_size = 12 \n",
    "block_size = 1024\n",
    "\n",
    "n_layer = 12\n",
    "n_head = 12\n",
    "n_embd = 768\n",
    "dropout = 0.0 \n",
    "bias = False \n",
    "\n",
    "learning_rate = 6e-4 \n",
    "max_iters = 600000 \n",
    "weight_decay = 1e-1\n",
    "beta1 = 0.9\n",
    "beta2 = 0.95\n",
    "grad_clip = 1.0 \n",
    "\n",
    "decay_lr = True \n",
    "warmup_iters = 2000 \n",
    "lr_decay_iters = 600000 \n",
    "min_lr = 6e-5 \n",
    "\n",
    "backend = 'nccl' \n",
    "device = 'cuda' \n",
    "dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' \n",
    "compile = True \n",
    "\n",
    "init_from = 'gpt2-large'\n",
    "\n",
    "model_args = dict(n_layer=n_layer, n_head=n_head, n_embd=n_embd, block_size=block_size,\n",
    "                  bias=bias, vocab_size=None, dropout=dropout) \n",
    "override_args = dict(dropout=dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ee830eb-0475-436d-a523-56c0978d8298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unique_modules(model):\n",
    "    module_names = []\n",
    "    for module in model.modules():\n",
    "        name = module.__class__.__name__\n",
    "\n",
    "        if name not in module_names:\n",
    "            module_names.append(name)\n",
    "            print(f'module: \\n{module}\\n\\n')\n",
    "        \n",
    "    return module_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ef67d6a8-d16e-4097-83de-163c554fc856",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_modules(\n",
    "    model,\n",
    "    ancestor_class: Optional[Set[str]] = None,\n",
    "    search_class: List[Type[nn.Module]] = [nn.Linear],\n",
    "    exclude_children_of: Optional[List[Type[nn.Module]]] = [\n",
    "        OFTInjectedLinear,\n",
    "        OFTInjectedConv2d,\n",
    "    ],\n",
    "):\n",
    "    \"\"\"\n",
    "    Find all modules of a certain class (or union of classes) that are direct or\n",
    "    indirect descendants of other modules of a certain class (or union of classes).\n",
    "    Returns all matching modules, along with the parent of those moduless and the\n",
    "    names they are referenced by.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the targets we should replace all linears under\n",
    "    if ancestor_class is not None:\n",
    "        ancestors = (\n",
    "            module\n",
    "            for module in model.modules()\n",
    "            if module.__class__.__name__ in ancestor_class\n",
    "        )\n",
    "    else:\n",
    "        # the first modules is the most senior father class.\n",
    "        # this, incase you want to naively iterate over all modules.\n",
    "        for module in model.modules():\n",
    "            ancestor_class = module.__class__.__name__\n",
    "            break\n",
    "        ancestors = (\n",
    "            module\n",
    "            for module in model.modules()\n",
    "            if module.__class__.__name__ in ancestor_class\n",
    "        )\n",
    "\n",
    "    results = []\n",
    "    # For each target find every linear_class module that isn't a child of a OFTInjectedLinear\n",
    "    for ancestor in ancestors:\n",
    "        for fullname, module in ancestor.named_modules():\n",
    "            if any([isinstance(module, _class) for _class in search_class]):\n",
    "                # Find the direct parent if this is a descendant, not a child, of target\n",
    "                *path, name = fullname.split(\".\")\n",
    "                parent = ancestor\n",
    "                while path:\n",
    "                    parent = parent.get_submodule(path.pop(0))\n",
    "                # Skip this linear if it's a child of a OFTInjectedLinear\n",
    "                if exclude_children_of and any(\n",
    "                    [isinstance(parent, _class) for _class in exclude_children_of]\n",
    "                ):\n",
    "                    continue\n",
    "                results.append((parent, name, module))  # Append the result to the list\n",
    "\n",
    "    return results  # Return the list instead of using 'yield'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0eb0f87d-d824-44d5-870f-67846296fcfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_oft(model):\n",
    "    target_replace_module = [\"CausalSelfAttention\"]\n",
    "    verbose = False\n",
    "    r = 4\n",
    "    eps = 1e-5\n",
    "    is_coft = True\n",
    "    block_share = False\n",
    "\n",
    "    collected_modules = []\n",
    "    module_count = 0\n",
    "\n",
    "    collected_dict = {}\n",
    "    names = []\n",
    "    require_grad_params = []\n",
    "\n",
    "    for _module, name, _child_module in _find_modules(\n",
    "            model, target_replace_module, search_class=[nn.Linear]\n",
    "        ):\n",
    "\n",
    "        if name not in collected_modules: \n",
    "            collected_modules.append(name)\n",
    "            collected_dict.update({name: 1})\n",
    "\n",
    "            print(f'Module name: {name}')\n",
    "            print(f'Module: {_module}')\n",
    "            print(f'Child module: {_child_module}\\n\\n')\n",
    "\n",
    "        collected_dict[name] += 1\n",
    "        module_count += 1\n",
    "\n",
    "        weight = _child_module.weight\n",
    "        bias = _child_module.bias\n",
    "\n",
    "        if verbose:\n",
    "            print(\"OFT Injection : injecting oft into \", name)\n",
    "            print(\"OFT Injection : weight shape\", weight.shape)\n",
    "\n",
    "        _tmp = OFTInjectedLinear(\n",
    "            _child_module.in_features,\n",
    "            _child_module.out_features,\n",
    "            _child_module.bias is not None,\n",
    "            r=r,\n",
    "            eps=eps,\n",
    "            is_coft=is_coft,\n",
    "            block_share=block_share,\n",
    "        )\n",
    "        _tmp.OFT.weight = weight\n",
    "        if bias is not None:\n",
    "            _tmp.OFT.bias = bias\n",
    "\n",
    "        _tmp.to(_child_module.weight.device).to(_child_module.weight.dtype)\n",
    "        _module._modules[name] = _tmp\n",
    "\n",
    "        require_grad_params.append(_module._modules[name].R)\n",
    "        _module._modules[name].R.requires_grad = True\n",
    "        names.append(name)\n",
    "    \n",
    "    \n",
    "    for key, value in collected_dict.items():\n",
    "        print(f\"Collected {value} '{key}' modules\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "671a1755-760c-4384-bcea-e624a89525d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_oft_params(model, print_shapes=False):\n",
    "    return get_params_by_name(model, print_shapes=print_shapes, name_filter=name_is_oft)\n",
    "\n",
    "def get_params_by_name(model, print_shapes=False, name_filter=None):\n",
    "    for n, p in model.named_parameters():\n",
    "        if name_filter is None or name_filter(n):\n",
    "            if print_shapes:\n",
    "                print(n, p.shape)\n",
    "            yield p\n",
    "\n",
    "def name_is_oft(name):\n",
    "    return (\"OFT\" in name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "230dbf22-15a2-451c-9e79-a25e9f18729c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model_type:  gpt2-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/alif/oft/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading weights from pre-trained GPT: gpt2-large\n",
      "Forcing vocab_size=50257, block_size=1024, bias=True (GPT checkpoint pre-reqs)\n",
      "Overriding dropout rate to 0.0\n"
     ]
    }
   ],
   "source": [
    "model = GPT.from_pretrained(init_from, override_args)\n",
    "for k in ['n_layer', 'n_head', 'n_embd', 'block_size', 'bias', 'vocab_size']:\n",
    "    model_args[k] = getattr(model.config, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1f2f251d-c859-4419-ba1c-e2f807239742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "module: \n",
      "GPT(\n",
      "  (transformer): ModuleDict(\n",
      "    (wte): Embedding(50257, 1280)\n",
      "    (wpe): Embedding(1024, 1280)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-35): 36 x Block(\n",
      "        (ln_1): LayerNorm()\n",
      "        (attn): CausalSelfAttention(\n",
      "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
      "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (ln_2): LayerNorm()\n",
      "        (mlp): MLP(\n",
      "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "          (gelu): GELU(approximate='none')\n",
      "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n",
      ")\n",
      "\n",
      "\n",
      "module: \n",
      "ModuleDict(\n",
      "  (wte): Embedding(50257, 1280)\n",
      "  (wpe): Embedding(1024, 1280)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0-35): 36 x Block(\n",
      "      (ln_1): LayerNorm()\n",
      "      (attn): CausalSelfAttention(\n",
      "        (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
      "        (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm()\n",
      "      (mlp): MLP(\n",
      "        (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "        (gelu): GELU(approximate='none')\n",
      "        (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm()\n",
      ")\n",
      "\n",
      "\n",
      "module: \n",
      "Embedding(50257, 1280)\n",
      "\n",
      "\n",
      "module: \n",
      "Dropout(p=0.0, inplace=False)\n",
      "\n",
      "\n",
      "module: \n",
      "ModuleList(\n",
      "  (0-35): 36 x Block(\n",
      "    (ln_1): LayerNorm()\n",
      "    (attn): CausalSelfAttention(\n",
      "      (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
      "      (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "      (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (ln_2): LayerNorm()\n",
      "    (mlp): MLP(\n",
      "      (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "      (gelu): GELU(approximate='none')\n",
      "      (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "module: \n",
      "Block(\n",
      "  (ln_1): LayerNorm()\n",
      "  (attn): CausalSelfAttention(\n",
      "    (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
      "    (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (ln_2): LayerNorm()\n",
      "  (mlp): MLP(\n",
      "    (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "    (gelu): GELU(approximate='none')\n",
      "    (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "module: \n",
      "LayerNorm()\n",
      "\n",
      "\n",
      "module: \n",
      "CausalSelfAttention(\n",
      "  (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
      "  (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "\n",
      "\n",
      "module: \n",
      "Linear(in_features=1280, out_features=3840, bias=True)\n",
      "\n",
      "\n",
      "module: \n",
      "MLP(\n",
      "  (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
      "  (gelu): GELU(approximate='none')\n",
      "  (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "\n",
      "\n",
      "module: \n",
      "GELU(approximate='none')\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "module_names = unique_modules(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22089a82-aca3-4eb9-aa9c-821f91277dec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['GPT',\n",
       " 'ModuleDict',\n",
       " 'Embedding',\n",
       " 'Dropout',\n",
       " 'ModuleList',\n",
       " 'Block',\n",
       " 'LayerNorm',\n",
       " 'CausalSelfAttention',\n",
       " 'Linear',\n",
       " 'MLP',\n",
       " 'GELU']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "module_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "079e9946-1bf2-4962-9a3d-68f56098fa9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from oft import OFTInjectedLinear, OFTInjectedConv2d, inject_trainable_oft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "240b7fd1-eb7d-4a93-8175-fdced333257d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 1280)\n",
       "    (wpe): Embedding(1024, 1280)\n",
       "    (dropout): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-35): 36 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=1280, out_features=3840, bias=True)\n",
       "          (c_proj): Linear(in_features=1280, out_features=1280, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=1280, out_features=5120, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=5120, out_features=1280, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=1280, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.requires_grad_(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cbb763ef-7c15-48bf-83c9-a1f37efcea25",
   "metadata": {},
   "outputs": [],
   "source": [
    "oft_modules = [\"CausalSelfAttention\"]\n",
    "device_type = 'cuda'\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=(dtype == 'float16'))\n",
    "model.to(device)\n",
    "oft_params, train_names = inject_trainable_oft(model, target_replace_module=oft_modules, verbose=False, r=4, eps=1e-3, is_coft=True, block_share=False)\n",
    "optimizer = model.configure_optimizers(weight_decay, learning_rate, (beta1, beta2), device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "888ee418-274e-4765-b4c2-7f4650be2a10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72\n",
      "param count: 29,491,200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(len(oft_params))\n",
    "\n",
    "param_count = 0\n",
    "for param in oft_params:\n",
    "    if param.requires_grad:\n",
    "        param_count += param.numel()\n",
    "        \n",
    "print(f'param count: {param_count:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a157f117-20fe-471e-8414-11fa82fa59cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_class = [nn.Linear]\n",
    "results = []\n",
    "exclude_children_of = None\n",
    "\n",
    "for elem in model.modules():\n",
    "    module_name = elem.__class__.__name__\n",
    "    if module_name in oft_modules:\n",
    "        ancestor = elem\n",
    "        \n",
    "        for fullname, module in ancestor.named_modules():\n",
    "            if any([isinstance(module, _class) for _class in search_class]):\n",
    "                \n",
    "                *path, name = fullname.split(\".\")\n",
    "                parent = ancestor\n",
    "                while path:\n",
    "                    parent = parent.get_submodule(path.pop(0))\n",
    "                # Skip this linear if it's a child of a OFTInjectedLinear\n",
    "                if exclude_children_of and any(\n",
    "                    [isinstance(parent, _class) for _class in exclude_children_of]\n",
    "                ):\n",
    "                    continue\n",
    "                results.append((parent, name, module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dc04cccf-7988-4a9b-a2bf-f32d17371412",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(OFTInjectedLinear(\n",
       "   (OFT): Linear(in_features=1280, out_features=3840, bias=True)\n",
       " ),\n",
       " 'OFT',\n",
       " Linear(in_features=1280, out_features=3840, bias=True))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f3197f2e-3e3d-4b7c-b08d-ba519423c2c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[ 0.1655, -0.2344,  0.1063,  ...,  0.0020, -0.1146,  0.0222],\n",
       "        [ 0.1230,  0.1413, -0.0397,  ...,  0.1257, -0.0897, -0.0171],\n",
       "        [ 0.1003,  0.0706,  0.1085,  ..., -0.0798, -0.0925, -0.0463],\n",
       "        ...,\n",
       "        [-0.0081, -0.0105, -0.0042,  ...,  0.0024,  0.0013,  0.0290],\n",
       "        [ 0.0106,  0.0239,  0.0183,  ...,  0.0351,  0.0007,  0.0258],\n",
       "        [-0.0183, -0.0101, -0.0080,  ...,  0.0204, -0.0041, -0.0327]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oft_linear = results[0][2]\n",
    "oft_linear.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "30ed0858-7025-40d7-b5e3-2430e93dbc20",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = get_batch(split)\n",
    "            with ctx:\n",
    "                logits, loss = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n",
    "\n",
    "def get_lr(it):\n",
    "\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * it / warmup_iters\n",
    "\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio)) \n",
    "    return min_lr + coeff * (learning_rate - min_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "d118d207-07da-4c05-96da-7c1161357c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 1 GPU...\n",
      "tokens per iteration will be: 491,520\n"
     ]
    }
   ],
   "source": [
    "ddp = int(os.environ.get('RANK', -1)) != -1 \n",
    "if ddp:\n",
    "    print(\"Initializing distributed training...\")\n",
    "    init_process_group(backend=backend)\n",
    "    ddp_rank = int(os.environ['RANK'])\n",
    "    ddp_local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    ddp_world_size = int(os.environ['WORLD_SIZE'])\n",
    "    device = f'cuda:{ddp_local_rank}'\n",
    "    torch.cuda.set_device(device)\n",
    "    master_process = ddp_rank == 0 \n",
    "    seed_offset = ddp_rank \n",
    "    assert gradient_accumulation_steps % ddp_world_size == 0\n",
    "    gradient_accumulation_steps //= ddp_world_size\n",
    "else:\n",
    "    print(\"Training on 1 GPU...\")\n",
    "    master_process = True\n",
    "    seed_offset = 0\n",
    "    ddp_world_size = 1\n",
    "tokens_per_iter = gradient_accumulation_steps * ddp_world_size * batch_size * block_size\n",
    "print(f\"tokens per iteration will be: {tokens_per_iter:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c7279f8d-5071-40db-a46f-02d90b6a578e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "data_dir = os.path.join('../data', dataset)\n",
    "train_data = np.memmap(os.path.join(data_dir, 'train.bin'), dtype=np.uint16, mode='r')\n",
    "val_data = np.memmap(os.path.join(data_dir, 'val.bin'), dtype=np.uint16, mode='r')\n",
    "def get_batch(split):\n",
    "    data = train_data if split == 'train' else val_data\n",
    "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
    "    x = torch.stack([torch.from_numpy((data[i:i+block_size]).astype(np.int64)) for i in ix])\n",
    "    y = torch.stack([torch.from_numpy((data[i+1:i+1+block_size]).astype(np.int64)) for i in ix])\n",
    "    if device_type == 'cuda':\n",
    "\n",
    "        x, y = x.pin_memory().to(device, non_blocking=True), y.pin_memory().to(device, non_blocking=True)\n",
    "    else:\n",
    "        x, y = x.to(device), y.to(device)\n",
    "    return x, y\n",
    "\n",
    "iter_num = 0\n",
    "best_val_loss = 1e9\n",
    "\n",
    "meta_path = os.path.join(data_dir, 'meta.pkl')\n",
    "meta_vocab_size = None\n",
    "if os.path.exists(meta_path):\n",
    "    with open(meta_path, 'rb') as f:\n",
    "        meta = pickle.load(f)\n",
    "    meta_vocab_size = meta['vocab_size']\n",
    "    print(f\"found vocab_size = {meta_vocab_size} (inside {meta_path})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1923bf13-d6c7-4664-b9f0-91a0184e470d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = get_batch('train') \n",
    "t0 = time.time()\n",
    "local_iter_num = 0 \n",
    "raw_model = model.module if ddp else model \n",
    "running_mfu = -1.0\n",
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "while True:\n",
    "\n",
    "    lr = get_lr(iter_num) if decay_lr else learning_rate\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "    if iter_num % eval_interval == 0 and master_process:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"step {iter_num}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
    "        \n",
    "        # sample_generation = model.generate(X, max_new_tokens=100)\n",
    "        # decoded_generation = enc.decode(sample_generation[0].tolist())\n",
    "        # print(f'\\n\\nSample Generation: \\n{decoded_generation}\\n\\n')\n",
    "        \n",
    "        if wandb_log:\n",
    "            wandb.log({\n",
    "                \"iter\": iter_num,\n",
    "                \"train/loss\": losses['train'],\n",
    "                \"val/loss\": losses['val'],\n",
    "                \"lr\": lr,\n",
    "                \"mfu\": running_mfu*100, # convert to percentage\n",
    "            })\n",
    "        \n",
    "        if losses['val'] < best_val_loss or always_save_checkpoint:\n",
    "            best_val_loss = losses['val']\n",
    "            if iter_num > 0:\n",
    "                checkpoint = {\n",
    "                    'model': raw_model.state_dict(),\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                    'model_args': model_args,\n",
    "                    'iter_num': iter_num,\n",
    "                    'best_val_loss': best_val_loss,\n",
    "                    'config': config,\n",
    "                }\n",
    "                if use_lora:\n",
    "                    checkpoint['lora'] = get_lora_state_dict(raw_model)\n",
    "                \n",
    "                print(f\"saving checkpoint to {out_dir}\")\n",
    "                checkpoint_name = dataset + '_' +  init_from + '_' + 'ckpt.pt'\n",
    "                torch.save(checkpoint, os.path.join(out_dir, checkpoint_name))\n",
    "    if iter_num == 0 and eval_only:\n",
    "        break\n",
    "\n",
    "    for micro_step in range(gradient_accumulation_steps):\n",
    "        if ddp:\n",
    "            model.require_backward_grad_sync = (micro_step == gradient_accumulation_steps - 1)\n",
    "        with ctx:\n",
    "            logits, loss = model(X, Y)\n",
    "            loss = loss / gradient_accumulation_steps \n",
    "\n",
    "        X, Y = get_batch('train')\n",
    "        scaler.scale(loss).backward()\n",
    "\n",
    "    if grad_clip != 0.0:\n",
    "        scaler.unscale_(optimizer)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip)\n",
    "\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    t1 = time.time()\n",
    "    dt = t1 - t0\n",
    "    t0 = t1\n",
    "    if iter_num % log_interval == 0 and master_process:\n",
    "        lossf = loss.item() * gradient_accumulation_steps\n",
    "        if local_iter_num >= 5: \n",
    "            mfu = raw_model.estimate_mfu(batch_size * gradient_accumulation_steps, dt)\n",
    "            running_mfu = mfu if running_mfu == -1.0 else 0.9*running_mfu + 0.1*mfu\n",
    "        print(f\"iter {iter_num}: loss {lossf:.4f}, time {dt*1000:.2f}ms, mfu {running_mfu*100:.2f}%\")\n",
    "    iter_num += 1\n",
    "    local_iter_num += 1\n",
    "\n",
    "    if iter_num > max_iters:\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
